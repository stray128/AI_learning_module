{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Part 4: GroupBy, Merge, and Pivot\n",
    "\n",
    "This notebook covers grouping, aggregation, merging, and reshaping data.\n",
    "\n",
    "**Topics covered:**\n",
    "- GroupBy operations\n",
    "- Aggregation functions\n",
    "- Merge and join\n",
    "- Concatenation\n",
    "- Pivot tables\n",
    "\n",
    "**Problems:** 15 (Easy: 1-5, Medium: 6-10, Hard: 11-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP - Run this cell first!\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utils.checker import check\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'dept': ['IT', 'HR', 'IT', 'HR', 'IT', 'Sales'],\n",
    "    'salary': [50000, 60000, 70000, 55000, 45000, 65000],\n",
    "    'years': [2, 5, 8, 3, 1, 4]\n",
    "})\n",
    "print(df)\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Basic GroupBy\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "`groupby()` groups rows by values in one or more columns, allowing you to perform aggregate operations on each group. This is fundamental for analyzing data by categories.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.groupby('column').agg_func()  # Group by column and aggregate\n",
    "df.groupby('column')['col2'].mean()  # Group and get mean of specific column\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.groupby('dept')['salary'].mean()\n",
    "dept\n",
    "HR      57500.0\n",
    "IT      55000.0\n",
    "Sales   65000.0\n",
    "```\n",
    "\n",
    "### Task\n",
    "Group `df` by 'dept' and get the mean salary for each department. Store in `dept_salary`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- Index should be department names\n",
    "- HR mean salary should be 57500.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "dept_salary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(dept_salary, pd.Series, \"P1: Type check\")\n",
    "check.is_true(abs(dept_salary['HR'] - 57500.0) < 0.1, \"P1: HR salary\", \"HR mean should be 57500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: GroupBy Count\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "The `count()` aggregation function returns the number of non-null values in each group. Alternatively, use `size()` to count all rows including nulls.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.groupby('column').size()       # Count all rows per group\n",
    "df.groupby('column')['col'].count()  # Count non-null values\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.groupby('dept').size()\n",
    "dept\n",
    "HR       2\n",
    "IT       3\n",
    "Sales    1\n",
    "```\n",
    "\n",
    "### Task\n",
    "Count the number of employees in each department. Store in `dept_count`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- IT should have 3 employees\n",
    "- Sales should have 1 employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "dept_count = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(dept_count, pd.Series, \"P2: Type check\")\n",
    "check.is_true(dept_count['IT'] == 3, \"P2: IT count\", \"IT should have 3 employees\")\n",
    "check.is_true(dept_count['Sales'] == 1, \"P2: Sales count\", \"Sales should have 1 employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Concatenate DataFrames Vertically\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "`concat()` combines multiple DataFrames. Vertical concatenation (axis=0) stacks DataFrames on top of each other, useful for combining data with the same structure.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.concat([df1, df2])              # Vertical (default axis=0)\n",
    "pd.concat([df1, df2], axis=0)      # Explicit vertical\n",
    "pd.concat([df1, df2], ignore_index=True)  # Reset index\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.concat([df1, df2])\n",
    "# Combines rows from both DataFrames\n",
    "```\n",
    "\n",
    "### Task\n",
    "Concatenate two DataFrames (`df1` and `df2`) vertically. Store in `concat_v`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 4 rows (2 from each DataFrame)\n",
    "- Should have columns ['A', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "# Your solution:\n",
    "concat_v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(concat_v, pd.DataFrame, \"P3: Type check\")\n",
    "check.has_length(concat_v, 4, \"P3: Length\")\n",
    "check.has_columns(concat_v, ['A', 'B'], \"P3: Columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Concatenate DataFrames Horizontally\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "Horizontal concatenation (axis=1) joins DataFrames side by side, adding columns. This is useful for combining different attributes of the same entities.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.concat([df1, df2], axis=1)  # Horizontal\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.concat([df1, df2], axis=1)\n",
    "   A  B  C  D\n",
    "0  1  3  5  7\n",
    "1  2  4  6  8\n",
    "```\n",
    "\n",
    "### Task\n",
    "Concatenate two DataFrames side by side (horizontally). Store in `concat_h`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 4 columns total\n",
    "- Columns should be ['A', 'B', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'C': [5, 6], 'D': [7, 8]})\n",
    "\n",
    "# Your solution:\n",
    "concat_h = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(concat_h, pd.DataFrame, \"P4: Type check\")\n",
    "check.is_true(len(concat_h.columns) == 4, \"P4: Four columns\", \"Should have 4 columns\")\n",
    "check.has_columns(concat_h, ['A', 'B', 'C', 'D'], \"P4: Columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Basic Merge (Inner Join)\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "`merge()` combines DataFrames based on common columns (like SQL joins). An inner join (default) keeps only rows with matching values in both DataFrames.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.merge(df1, df2, on='column')           # Inner join on column\n",
    "pd.merge(df1, df2, how='inner')           # Explicit inner\n",
    "df1.merge(df2, on='column')               # Alternative syntax\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.merge(employees, salaries, on='emp_id')\n",
    "# Only employees with salary records\n",
    "```\n",
    "\n",
    "### Task\n",
    "Merge two DataFrames on the 'emp_id' column (inner join). Store in `merged`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 2 rows (only matching emp_ids: 1 and 2)\n",
    "- Should contain 'salary' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "employees = pd.DataFrame({'emp_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
    "salaries = pd.DataFrame({'emp_id': [1, 2, 4], 'salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Your solution:\n",
    "merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(merged, pd.DataFrame, \"P5: Type check\")\n",
    "check.has_length(merged, 2, \"P5: Length\")\n",
    "check.contains_column(merged, 'salary', \"P5: Has salary column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: GroupBy with Multiple Aggregations\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "You can apply multiple aggregation functions at once using `agg()` with a list of function names. This provides comprehensive summaries of each group.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.groupby('column')['col2'].agg(['mean', 'sum', 'count'])\n",
    "df.groupby('column').agg({'col1': 'mean', 'col2': 'sum'})\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.groupby('dept')['salary'].agg(['mean', 'sum'])\n",
    "         mean     sum\n",
    "dept                 \n",
    "HR     57500  115000\n",
    "IT     55000  165000\n",
    "```\n",
    "\n",
    "### Task\n",
    "Group by 'dept' and calculate both mean and sum of salary. Store in `dept_agg`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 2 columns (mean and sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "dept_agg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(dept_agg, pd.DataFrame, \"P6: Type check\")\n",
    "check.is_true(len(dept_agg.columns) == 2, \"P6: Two columns\", \"Should have 2 aggregation columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 7: Left Merge\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "A left merge keeps all rows from the left DataFrame and matches rows from the right. Unmatched rows get NaN for right DataFrame columns.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.merge(df1, df2, on='column', how='left')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.merge(employees, salaries, on='emp_id', how='left')\n",
    "# All employees, salary=NaN if no match\n",
    "```\n",
    "\n",
    "### Task\n",
    "Perform a left merge to keep all employees even if they don't have salary records. Store in `left_merged`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 3 rows (all employees)\n",
    "- Charlie should have NaN salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "employees = pd.DataFrame({'emp_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
    "salaries = pd.DataFrame({'emp_id': [1, 2, 4], 'salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Your solution:\n",
    "left_merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(left_merged, pd.DataFrame, \"P7: Type check\")\n",
    "check.has_length(left_merged, 3, \"P7: Length\")\n",
    "check.is_true(pd.isna(left_merged[left_merged['name'] == 'Charlie']['salary'].iloc[0]), \"P7: Charlie has NaN\", \"Charlie should have NaN salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 8: GroupBy Transform\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "`transform()` applies a function to each group but returns a Series with the same shape as the original DataFrame. It's used to add group statistics to each row.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['new_col'] = df.groupby('column')['col2'].transform('mean')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df['dept_avg'] = df.groupby('dept')['salary'].transform('mean')\n",
    "# Each row gets its department's average salary\n",
    "```\n",
    "\n",
    "### Task\n",
    "Add a column 'dept_avg_salary' showing the department's average salary for each row. Store the result DataFrame in `df_with_avg`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should contain 'dept_avg_salary' column\n",
    "- Should have 6 rows (same as original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "df_with_avg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(df_with_avg, pd.DataFrame, \"P8: Type check\")\n",
    "check.contains_column(df_with_avg, 'dept_avg_salary', \"P8: Has dept_avg_salary column\")\n",
    "check.has_length(df_with_avg, 6, \"P8: Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 9: Value Counts\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "`value_counts()` counts the frequency of each unique value in a Series. It's a quick way to understand the distribution of categorical data.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['column'].value_counts()           # Counts in descending order\n",
    "df['column'].value_counts(normalize=True)  # As percentages\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df['dept'].value_counts()\n",
    "IT       3\n",
    "HR       2\n",
    "Sales    1\n",
    "```\n",
    "\n",
    "### Task\n",
    "Count occurrences of each department. Store in `dept_counts`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- IT should have 3 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "dept_counts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(dept_counts, pd.Series, \"P9: Type check\")\n",
    "check.is_true(dept_counts['IT'] == 3, \"P9: IT count\", \"IT should have 3 occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 10: Simple Pivot Table\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Pivot tables reshape data by creating a matrix where rows and columns are defined by categorical variables, and cells contain aggregated values.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.pivot_table(values='value_col', index='row_col', aggfunc='mean')\n",
    "df.pivot_table(values='value_col', index='row_col', columns='col_col')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.pivot_table(values='salary', index='dept', aggfunc='mean')\n",
    "         salary\n",
    "dept           \n",
    "HR      57500.0\n",
    "IT      55000.0\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a pivot table showing average salary by department. Store in `pivot`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- IT salary should be approximately 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "pivot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(pivot, pd.DataFrame, \"P10: Type check\")\n",
    "check.is_true(abs(pivot.loc['IT', 'salary'] - 55000.0) < 100, \"P10: IT salary\", \"IT salary should be around 55000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 11: GroupBy with Multiple Columns\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "You can group by multiple columns to create hierarchical groups. This creates a multi-index Series or DataFrame showing nested aggregations.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.groupby(['col1', 'col2'])['col3'].mean()\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.groupby(['dept', 'experienced'])['salary'].mean()\n",
    "dept   experienced\n",
    "HR     False          60000\n",
    "       True           55000\n",
    "IT     False          47500\n",
    "       True           70000\n",
    "```\n",
    "\n",
    "### Task\n",
    "The setup creates an 'experienced' column. Group by both 'dept' and 'experienced' (years > 3), then get mean salary. Store in `grouped_multi`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- Should have multi-level index\n",
    "- Should have at least 3 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "df_exp = df.copy()\n",
    "df_exp['experienced'] = df_exp['years'] > 3\n",
    "\n",
    "# Your solution:\n",
    "grouped_multi = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(grouped_multi, pd.Series, \"P11: Type check\")\n",
    "check.is_true(len(grouped_multi) >= 3, \"P11: Multiple groups\", \"Should have at least 3 groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 12: Outer Merge\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "An outer (full) merge keeps all rows from both DataFrames, filling with NaN where there's no match. This is useful when you want to see all records from both sources.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.merge(df1, df2, on='column', how='outer')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.merge(employees, salaries, on='emp_id', how='outer')\n",
    "# All employees AND all salary records\n",
    "```\n",
    "\n",
    "### Task\n",
    "Perform an outer merge to keep all records from both DataFrames. Store in `outer_merged`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 4 rows (all unique emp_ids: 1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "employees = pd.DataFrame({'emp_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
    "salaries = pd.DataFrame({'emp_id': [1, 2, 4], 'salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Your solution:\n",
    "outer_merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(outer_merged, pd.DataFrame, \"P12: Type check\")\n",
    "check.has_length(outer_merged, 4, \"P12: Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 13: Crosstab\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "`crosstab()` creates a frequency table showing counts of combinations between two or more categorical variables. It's like a pivot table specifically for counting.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.crosstab(df['col1'], df['col2'])\n",
    "pd.crosstab(df['col1'], df['col2'], values=df['col3'], aggfunc='mean')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> pd.crosstab(df['dept'], df['experienced'])\n",
    "experienced  False  True\n",
    "dept                    \n",
    "HR               1     1\n",
    "IT               2     1\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a cross-tabulation of department and experienced status. Store in `crosstab`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Total of all values should be 6 (total employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "df_exp = df.copy()\n",
    "df_exp['experienced'] = df_exp['years'] > 3\n",
    "\n",
    "# Your solution:\n",
    "crosstab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(crosstab, pd.DataFrame, \"P13: Type check\")\n",
    "check.is_true(crosstab.sum().sum() == 6, \"P13: Total count\", \"Total should be 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 14: Melt (Unpivot)\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "`melt()` transforms wide-format data to long-format, converting column names into a variable column and their values into a value column. This is the opposite of pivot.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.melt(id_vars=['id_col'], value_vars=['col1', 'col2'])\n",
    "df.melt(id_vars=['id_col'], var_name='variable', value_name='value')\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> wide_df.melt(id_vars=['name'], value_vars=['Q1', 'Q2'])\n",
    "    name variable  value\n",
    "0  Alice       Q1    100\n",
    "1    Bob       Q1    200\n",
    "2  Alice       Q2    150\n",
    "3    Bob       Q2    250\n",
    "```\n",
    "\n",
    "### Task\n",
    "Convert wide format to long format by melting Q1, Q2, Q3 into 'quarter' and 'value' columns. Store in `melted`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas DataFrame\n",
    "- Should have 6 rows (2 names Ã— 3 quarters)\n",
    "- Should contain 'value' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "wide_df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob'],\n",
    "    'Q1': [100, 200],\n",
    "    'Q2': [150, 250],\n",
    "    'Q3': [200, 300]\n",
    "})\n",
    "\n",
    "# Your solution:\n",
    "melted = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(melted, pd.DataFrame, \"P14: Type check\")\n",
    "check.has_length(melted, 6, \"P14: Length\")\n",
    "check.contains_column(melted, 'value', \"P14: Has value column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 15: GroupBy Apply Custom Function\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "`apply()` with groupby allows you to apply custom functions to each group. This is powerful for complex aggregations that aren't built-in.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "def custom_func(group):\n",
    "    return group['col'].max() - group['col'].min()\n",
    "\n",
    "df.groupby('column').apply(custom_func)\n",
    "df.groupby('column')['col2'].apply(lambda x: x.max() - x.min())\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.groupby('dept')['salary'].apply(lambda x: x.max() - x.min())\n",
    "dept\n",
    "HR        5000\n",
    "IT       25000\n",
    "Sales        0\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply a custom function to each group: return the range (max - min) of salary per department. Store in `salary_range`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- IT range should be 25000 (70000 - 45000)\n",
    "- Sales range should be 0 (only one employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "salary_range = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(salary_range, pd.Series, \"P15: Type check\")\n",
    "check.is_true(salary_range['IT'] == 25000, \"P15: IT range\", \"IT range should be 25000\")\n",
    "check.is_true(salary_range['Sales'] == 0, \"P15: Sales range\", \"Sales range should be 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Run this cell to see your overall progress on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
