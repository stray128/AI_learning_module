{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Statistics - Part 2: Preprocessing\n",
    "\n",
    "This notebook covers data preprocessing techniques using sklearn.\n",
    "\n",
    "**Topics covered:**\n",
    "- StandardScaler (z-score normalization)\n",
    "- MinMaxScaler (0-1 scaling)\n",
    "- RobustScaler (median/IQR scaling)\n",
    "- Normalization (L1, L2)\n",
    "- Imputation (handling missing values)\n",
    "\n",
    "**Problems:** 17 (Easy: 1-6, Medium: 7-12, Hard: 13-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SETUP - Run this cell first!\n# ============================================\nimport sys\nsys.path.insert(0, '..')\nfrom utils.checker import check\n\nprint(\"Checker loaded! Now import the libraries you need.\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Problem 0: Import Required Libraries\n**Difficulty:** Easy\n\n### Concept\nBefore preprocessing data, you need to import the necessary libraries. NumPy and Pandas handle data structures, while sklearn provides preprocessing utilities.\n\n### Syntax\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer\nfrom sklearn.impute import SimpleImputer\n```\n\n### Task\nImport the following:\n- NumPy as `np`\n- Pandas as `pd`\n- From sklearn.preprocessing: StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer\n- From sklearn.impute: SimpleImputer\n\n### Expected Properties\n- All modules and classes should be importable",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your solution:\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Verification\ncheck.is_true('np' in dir(), \"P0a: NumPy imported\", \"Import numpy as np\")\ncheck.is_true('pd' in dir(), \"P0b: Pandas imported\", \"Import pandas as pd\")\ncheck.is_true('StandardScaler' in dir(), \"P0c: StandardScaler imported\", \"Import StandardScaler from sklearn.preprocessing\")\ncheck.is_true('SimpleImputer' in dir(), \"P0d: SimpleImputer imported\", \"Import SimpleImputer from sklearn.impute\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: StandardScaler - Fit and Transform\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "StandardScaler transforms features to have zero mean and unit variance (z-score normalization). This is crucial for algorithms sensitive to feature scales like SVM, KNN, and neural networks.\n",
    "\n",
    "Formula: `z = (x - mean) / std`\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Or in two steps\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[0], [5], [10]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> scaler.fit_transform(X)\n",
    "array([[-1.22],\n",
    "       [ 0.  ],\n",
    "       [ 1.22]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply StandardScaler to the data `[[10], [20], [30], [40], [50]]`. Store the scaled result in `X_scaled`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_scaled` should be a numpy array\n",
    "- Mean of scaled data should be approximately 0\n",
    "- Standard deviation of scaled data should be approximately 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "X_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_scaled, \"P1: Not None\")\n",
    "check.is_type(X_scaled, np.ndarray, \"P1: Type check\")\n",
    "check.has_shape(X_scaled, (5, 1), \"P1: Correct shape\")\n",
    "check.mean_is_close(X_scaled, 0.0, \"P1a: Mean is 0\", tolerance=0.01)\n",
    "check.std_is_close(X_scaled, 1.0, \"P1b: Std is 1\", tolerance=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: MinMaxScaler - Scale to 0-1\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "MinMaxScaler transforms features to a fixed range, typically [0, 1]. This is useful when features need to be bounded, such as for neural network inputs or when you want to preserve zero entries in sparse data.\n",
    "\n",
    "Formula: `X_scaled = (X - X.min) / (X.max - X.min)`\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()  # Default range [0, 1]\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[0], [5], [10]]\n",
    ">>> scaler = MinMaxScaler()\n",
    ">>> scaler.fit_transform(X)\n",
    "array([[0. ],\n",
    "       [0.5],\n",
    "       [1. ]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply MinMaxScaler to scale data to the range [0, 1]. Store in `X_minmax`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_minmax` should be a numpy array\n",
    "- Minimum value should be 0\n",
    "- Maximum value should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "X_minmax = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_minmax, \"P2: Not None\")\n",
    "check.is_type(X_minmax, np.ndarray, \"P2: Type check\")\n",
    "check.has_shape(X_minmax, (5, 1), \"P2: Correct shape\")\n",
    "check.min_value_is(X_minmax, 0.0, \"P2a: Min is 0\")\n",
    "check.max_value_is(X_minmax, 1.0, \"P2b: Max is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: MinMaxScaler - Custom Range\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "MinMaxScaler can scale to any custom range using the `feature_range` parameter. This is useful when you need specific bounds, like [-1, 1] for certain activation functions.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[0], [5], [10]]\n",
    ">>> scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    ">>> scaler.fit_transform(X)\n",
    "array([[-1.],\n",
    "       [ 0.],\n",
    "       [ 1.]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Scale data to range [-1, 1] using MinMaxScaler with `feature_range=(-1, 1)`. Store in `X_custom`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_custom` should be a numpy array\n",
    "- Minimum value should be -1\n",
    "- Maximum value should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "X_custom = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_custom, \"P3: Not None\")\n",
    "check.is_type(X_custom, np.ndarray, \"P3: Type check\")\n",
    "check.min_value_is(X_custom, -1.0, \"P3a: Min is -1\")\n",
    "check.max_value_is(X_custom, 1.0, \"P3b: Max is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Get Scaler Parameters\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "After fitting a scaler, you can access the learned parameters. For StandardScaler, `mean_` contains the mean and `scale_` contains the standard deviation used for transformation.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "mean = scaler.mean_      # Mean of each feature\n",
    "std = scaler.scale_      # Std of each feature\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[10], [20], [30]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> scaler.fit(X)\n",
    ">>> scaler.mean_\n",
    "array([20.])\n",
    ">>> scaler.scale_\n",
    "array([8.16...])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fit a StandardScaler on the data and extract the mean and standard deviation. Store them in `mean_` and `std_`.\n",
    "\n",
    "### Expected Properties\n",
    "- `mean_` should be an array with the mean value\n",
    "- `std_` should be an array with the standard deviation\n",
    "- Mean should be 30 (middle of 10-50)\n",
    "- Std should be approximately 14.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "mean_ = None\n",
    "std_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(mean_, \"P4a: Mean not None\")\n",
    "check.is_not_none(std_, \"P4b: Std not None\")\n",
    "check.is_type(mean_, np.ndarray, \"P4c: Mean type check\")\n",
    "check.is_type(std_, np.ndarray, \"P4d: Std type check\")\n",
    "check.is_true(abs(mean_[0] - 30.0) < 0.01, \"P4e: Correct mean\", \"Mean should be 30\")\n",
    "check.value_in_range(std_[0], 14, 15, \"P4f: Reasonable std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Simple Imputer - Mean\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "Missing values (NaN) must be handled before applying most machine learning algorithms. SimpleImputer fills missing values with a statistic (mean, median, most frequent, or constant).\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent', 'constant'\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [2], [np.nan], [4]]\n",
    ">>> imputer = SimpleImputer(strategy='mean')\n",
    ">>> imputer.fit_transform(X)\n",
    "array([[1. ],\n",
    "       [2. ],\n",
    "       [2.33],  # Mean of 1, 2, 4\n",
    "       [4. ]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fill missing values with the mean using SimpleImputer. Store in `X_imputed`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_imputed` should be a numpy array\n",
    "- Should have no NaN values\n",
    "- The missing value should be replaced with the mean of non-missing values (3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[1], [2], [np.nan], [4], [5]])\n",
    "X_imputed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_imputed, \"P5: Not None\")\n",
    "check.is_type(X_imputed, np.ndarray, \"P5: Type check\")\n",
    "check.has_shape(X_imputed, (5, 1), \"P5: Correct shape\")\n",
    "check.is_true(not np.any(np.isnan(X_imputed)), \"P5a: No NaN values\", \"Should have no missing values\")\n",
    "check.is_true(abs(X_imputed[2, 0] - 3.0) < 0.01, \"P5b: Correct imputation\", \"Missing value should be replaced with mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: Simple Imputer - Median\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "The median strategy is more robust to outliers than the mean. When data has extreme values, median imputation preserves the central tendency better.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [2], [np.nan], [100]]  # 100 is outlier\n",
    ">>> imputer = SimpleImputer(strategy='median')\n",
    ">>> imputer.fit_transform(X)\n",
    "array([[  1.],\n",
    "       [  2.],\n",
    "       [  1.5],  # Median of 1, 2, 100 = 2\n",
    "       [100.]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fill missing values with the median. Note the outlier value 100. Store in `X_imputed`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_imputed` should be a numpy array\n",
    "- Should have no NaN values\n",
    "- Missing value should be replaced with median (3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[1], [2], [np.nan], [4], [100]])  # 100 is outlier\n",
    "X_imputed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_imputed, \"P6: Not None\")\n",
    "check.is_type(X_imputed, np.ndarray, \"P6: Type check\")\n",
    "check.is_true(not np.any(np.isnan(X_imputed)), \"P6a: No NaN values\", \"Should have no missing values\")\n",
    "check.is_true(abs(X_imputed[2, 0] - 3.0) < 0.01, \"P6b: Correct median imputation\", \"Should use median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 7: RobustScaler\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "RobustScaler uses the median and IQR (Interquartile Range) instead of mean and std. This makes it robust to outliers, as extreme values don't affect the median and IQR as much as they affect mean and std.\n",
    "\n",
    "Formula: `X_scaled = (X - median) / IQR`\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [2], [3], [100]]  # 100 is outlier\n",
    ">>> scaler = RobustScaler()\n",
    ">>> scaler.fit_transform(X)\n",
    "# Outlier doesn't dominate the scaling\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply RobustScaler to data that contains an outlier (100). Store in `X_robust`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_robust` should be a numpy array\n",
    "- Should have same shape as input\n",
    "- Median should map to approximately 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[1], [2], [3], [4], [5], [100]])  # 100 is outlier\n",
    "X_robust = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_robust, \"P7: Not None\")\n",
    "check.is_type(X_robust, np.ndarray, \"P7: Type check\")\n",
    "check.has_shape(X_robust, (6, 1), \"P7: Correct shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 8: Scale Multiple Features\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "StandardScaler works on each feature (column) independently. When you have multiple features with different scales, StandardScaler normalizes each one separately.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "# X is 2D array with multiple columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1, 100],\n",
    "...      [2, 200],\n",
    "...      [3, 300]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> scaler.fit_transform(X)\n",
    "# Each column scaled independently\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply StandardScaler to the multi-feature dataset. Each feature has a different scale. Store in `X_scaled`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_scaled` should be a numpy array\n",
    "- Should have shape (5, 3)\n",
    "- Mean of each column should be approximately 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([\n",
    "    [10, 100, 1000],\n",
    "    [20, 200, 2000],\n",
    "    [30, 300, 3000],\n",
    "    [40, 400, 4000],\n",
    "    [50, 500, 5000]\n",
    "])\n",
    "\n",
    "X_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_scaled, \"P8: Not None\")\n",
    "check.is_type(X_scaled, np.ndarray, \"P8: Type check\")\n",
    "check.has_shape(X_scaled, (5, 3), \"P8: Correct shape\")\n",
    "check.mean_is_close(X_scaled.mean(axis=0), 0.0, \"P8: Column means are 0\", tolerance=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 9: L2 Normalization\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Normalization (not to be confused with standardization) scales each **sample** (row) to have unit norm. L2 normalization divides each sample by its Euclidean length, making each row have length 1.\n",
    "\n",
    "This is useful for algorithms that measure similarity (like cosine similarity) or when the magnitude of vectors doesn't matter.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer(norm='l2')  # or 'l1', 'max'\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[3, 4]]  # Length = sqrt(3^2 + 4^2) = 5\n",
    ">>> normalizer = Normalizer(norm='l2')\n",
    ">>> normalizer.fit_transform(X)\n",
    "array([[0.6, 0.8]])  # [3/5, 4/5]\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply L2 normalization so each sample (row) has unit norm. Store in `X_normalized`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_normalized` should be a numpy array\n",
    "- Each row should have L2 norm of 1\n",
    "- Shape should be (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([\n",
    "    [3, 4],\n",
    "    [6, 8],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "X_normalized = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_normalized, \"P9: Not None\")\n",
    "check.is_type(X_normalized, np.ndarray, \"P9: Type check\")\n",
    "check.has_shape(X_normalized, (3, 2), \"P9: Correct shape\")\n",
    "_row_norms = np.linalg.norm(X_normalized, axis=1)\n",
    "check.is_true(np.allclose(_row_norms, 1.0), \"P9: Unit norm\", \"Each row should have L2 norm of 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 10: Imputer with Constant Value\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Sometimes you want to fill missing values with a specific constant rather than a statistic. This is useful when a specific value has semantic meaning (e.g., 0 for \"no information\").\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [np.nan], [3]]\n",
    ">>> imputer = SimpleImputer(strategy='constant', fill_value=-999)\n",
    ">>> imputer.fit_transform(X)\n",
    "array([[   1.],\n",
    "       [-999.],\n",
    "       [   3.]])\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fill missing values with constant value 0. Store in `X_imputed`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_imputed` should be a numpy array\n",
    "- Should have no NaN values\n",
    "- Missing values should be replaced with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[1], [2], [np.nan], [4], [np.nan]])\n",
    "X_imputed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_imputed, \"P10: Not None\")\n",
    "check.is_type(X_imputed, np.ndarray, \"P10: Type check\")\n",
    "check.is_true(not np.any(np.isnan(X_imputed)), \"P10a: No NaN values\", \"Should have no missing values\")\n",
    "check.is_true(X_imputed[2, 0] == 0.0, \"P10b: First imputation\", \"Should be filled with 0\")\n",
    "check.is_true(X_imputed[4, 0] == 0.0, \"P10c: Second imputation\", \"Should be filled with 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 11: Fit on Train, Transform Test\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "**CRITICAL**: When preprocessing, you must fit the scaler ONLY on training data, then apply the same transformation to both train and test. This prevents data leakage - the test set must not influence the transformation parameters.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Learn parameters from train only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same transformation\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X_train = [[1], [2], [3]]\n",
    ">>> X_test = [[4]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> scaler.fit(X_train)  # mean=2, std=1\n",
    ">>> scaler.transform(X_test)  # Uses train's mean and std\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fit StandardScaler on training data, then transform both train and test sets. Store in `X_train_scaled` and `X_test_scaled`.\n",
    "\n",
    "### Expected Properties\n",
    "- Both should be numpy arrays\n",
    "- Both should be transformed using the same parameters\n",
    "- Test set values can be outside [-3, 3] range since they use train statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X_train = np.array([[10], [20], [30], [40], [50]])\n",
    "X_test = np.array([[15], [25], [60]])  # Note: 60 is outside training range\n",
    "\n",
    "X_train_scaled = None\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_train_scaled, \"P11a: Train not None\")\n",
    "check.is_not_none(X_test_scaled, \"P11b: Test not None\")\n",
    "check.is_type(X_train_scaled, np.ndarray, \"P11c: Train type check\")\n",
    "check.is_type(X_test_scaled, np.ndarray, \"P11d: Test type check\")\n",
    "check.has_shape(X_train_scaled, (5, 1), \"P11e: Train shape\")\n",
    "check.has_shape(X_test_scaled, (3, 1), \"P11f: Test shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 12: L1 Normalization\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "L1 normalization scales each sample so that the sum of absolute values equals 1. This is useful for sparse data or when you want each sample to represent a probability distribution.\n",
    "\n",
    "Formula: `X_normalized[i] = X[i] / sum(|X[i]|)`\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "normalizer = Normalizer(norm='l1')\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1, 2, 3]]  # Sum = 6\n",
    ">>> normalizer = Normalizer(norm='l1')\n",
    ">>> normalizer.fit_transform(X)\n",
    "array([[0.167, 0.333, 0.5]])  # [1/6, 2/6, 3/6]\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply L1 normalization so each row sums to 1 (in absolute value). Store in `X_l1`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_l1` should be a numpy array\n",
    "- Sum of absolute values in each row should be 1\n",
    "- Shape should be (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "X_l1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_l1, \"P12: Not None\")\n",
    "check.is_type(X_l1, np.ndarray, \"P12: Type check\")\n",
    "check.has_shape(X_l1, (2, 3), \"P12: Correct shape\")\n",
    "_row_sums = np.abs(X_l1).sum(axis=1)\n",
    "check.is_true(np.allclose(_row_sums, 1.0), \"P12: L1 norm\", \"Each row should sum to 1 in absolute value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 13: Inverse Transform\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "After scaling, you can recover the original values using `inverse_transform()`. This is useful for interpreting results or converting predictions back to the original scale.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_original = scaler.inverse_transform(X_scaled)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[10], [20], [30]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> X_scaled = scaler.fit_transform(X)\n",
    ">>> scaler.inverse_transform(X_scaled)\n",
    "array([[10.],\n",
    "       [20.],\n",
    "       [30.]])  # Back to original\n",
    "```\n",
    "\n",
    "### Task\n",
    "Scale the data with StandardScaler, then use `inverse_transform()` to recover the original values. Store in `X_original`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_original` should be a numpy array\n",
    "- Should match the original X values (within floating point precision)\n",
    "- Shape should be (5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_original = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_original, \"P13: Not None\")\n",
    "check.is_type(X_original, np.ndarray, \"P13: Type check\")\n",
    "check.has_shape(X_original, (5, 1), \"P13: Correct shape\")\n",
    "check.is_true(np.allclose(X_original, X), \"P13: Recovered original\", \"Should match original values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 14: Impute Multiple Columns\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "SimpleImputer can handle DataFrames with multiple columns, imputing each column independently. When working with DataFrames, you often want to preserve the column names and structure.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(df)\n",
    "df_imputed = pd.DataFrame(X_imputed, columns=df.columns)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df = pd.DataFrame({'A': [1, np.nan, 3], 'B': [4, 5, np.nan]})\n",
    ">>> imputer = SimpleImputer(strategy='mean')\n",
    ">>> df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "```\n",
    "\n",
    "### Task\n",
    "Impute missing values in the DataFrame using mean strategy. Store as a DataFrame in `df_imputed`.\n",
    "\n",
    "### Expected Properties\n",
    "- `df_imputed` should be a DataFrame\n",
    "- Should have no missing values\n",
    "- Should have same shape as original (5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, 30, 40, 50],\n",
    "    'C': [100, 200, 300, np.nan, 500]\n",
    "})\n",
    "\n",
    "df_imputed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(df_imputed, \"P14: Not None\")\n",
    "check.is_type(df_imputed, pd.DataFrame, \"P14: Type check\")\n",
    "check.has_shape(df_imputed, (5, 3), \"P14: Correct shape\")\n",
    "check.has_no_nulls(df_imputed, \"P14: No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 15: Compare Scalers\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "Different scalers behave differently with outliers:\n",
    "- StandardScaler: Sensitive to outliers (uses mean/std)\n",
    "- MinMaxScaler: Very sensitive to outliers (uses min/max)\n",
    "- RobustScaler: Robust to outliers (uses median/IQR)\n",
    "\n",
    "Understanding these differences helps you choose the right scaler.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "X_standard = StandardScaler().fit_transform(X)\n",
    "X_minmax = MinMaxScaler().fit_transform(X)\n",
    "X_robust = RobustScaler().fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [2], [3], [100]]  # Has outlier\n",
    ">>> # StandardScaler will be affected by 100\n",
    ">>> # RobustScaler will handle it better\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply all three scalers to data with an outlier. Store results in `X_standard`, `X_minmax`, and `X_robust`.\n",
    "\n",
    "### Expected Properties\n",
    "- All three should be numpy arrays\n",
    "- All should have shape (6, 1)\n",
    "- RobustScaler will handle the outlier (1000) better than the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[1], [2], [3], [4], [5], [1000]])  # Has outlier\n",
    "\n",
    "X_standard = None\n",
    "X_minmax = None\n",
    "X_robust = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_standard, \"P15a: Standard not None\")\n",
    "check.is_not_none(X_minmax, \"P15b: MinMax not None\")\n",
    "check.is_not_none(X_robust, \"P15c: Robust not None\")\n",
    "check.has_shape(X_standard, (6, 1), \"P15d: Standard shape\")\n",
    "check.has_shape(X_minmax, (6, 1), \"P15e: MinMax shape\")\n",
    "check.has_shape(X_robust, (6, 1), \"P15f: Robust shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 16: MaxAbsScaler\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "MaxAbsScaler scales each feature by dividing by the maximum absolute value. This scales data to the range [-1, 1] while preserving sparsity (zeros remain zeros). Useful for sparse data.\n",
    "\n",
    "Formula: `X_scaled = X / max(|X|)`\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[-10], [5], [20]]\n",
    ">>> scaler = MaxAbsScaler()\n",
    ">>> scaler.fit_transform(X)\n",
    "array([[-0.5],  # -10/20\n",
    "       [ 0.25], #   5/20\n",
    "       [ 1.  ]]) #  20/20\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply MaxAbsScaler to scale data to [-1, 1] based on maximum absolute value. Store in `X_maxabs`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_maxabs` should be a numpy array\n",
    "- Maximum absolute value should be 1.0\n",
    "- Values should be in range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "X = np.array([[-10], [5], [20], [-5], [10]])\n",
    "X_maxabs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_maxabs, \"P16: Not None\")\n",
    "check.is_type(X_maxabs, np.ndarray, \"P16: Type check\")\n",
    "check.has_shape(X_maxabs, (5, 1), \"P16: Correct shape\")\n",
    "check.is_true(abs(np.abs(X_maxabs).max() - 1.0) < 0.01, \"P16: Max abs is 1\", \"Maximum absolute value should be 1\")\n",
    "check.all_values_in_range(X_maxabs, -1, 1, \"P16: In range [-1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 17: Power Transform (Yeo-Johnson)\n",
    "**Difficulty:** Hard\n",
    "\n",
    "### Concept\n",
    "Power transformations make data more Gaussian-like (normal distribution). This is useful for many statistical methods that assume normality. Yeo-Johnson can handle both positive and negative values (unlike Box-Cox which requires positive values).\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X_transformed = pt.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> X = [[1], [2], [3], [100]]  # Skewed data\n",
    ">>> pt = PowerTransformer(method='yeo-johnson')\n",
    ">>> X_transformed = pt.fit_transform(X)\n",
    ">>> # Now more normally distributed\n",
    "```\n",
    "\n",
    "### Task\n",
    "Apply Yeo-Johnson power transformation to make skewed data more Gaussian. Store in `X_power`.\n",
    "\n",
    "### Expected Properties\n",
    "- `X_power` should be a numpy array\n",
    "- Should have same shape as input (7, 1)\n",
    "- Data should be more normally distributed than original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "# Skewed data\n",
    "X = np.array([[1], [2], [3], [4], [5], [100], [200]])\n",
    "\n",
    "X_power = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(X_power, \"P17: Not None\")\n",
    "check.is_type(X_power, np.ndarray, \"P17: Type check\")\n",
    "check.has_shape(X_power, (7, 1), \"P17: Correct shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Run this cell to see your overall progress on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}