{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Capstone Project 2: Real-World Data Cleaning Challenge\n",
    "\n",
    "This project guides you through cleaning a messy dataset with real-world issues.\n",
    "\n",
    "**Objectives:**\n",
    "- Handle various types of missing data\n",
    "- Fix inconsistent data formats\n",
    "- Deal with outliers\n",
    "- Standardize categories\n",
    "- Create a clean, analysis-ready dataset\n",
    "\n",
    "**Problems:** 15 (Progressive difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SETUP - Run this cell first!\n# ============================================\nimport sys\nsys.path.insert(0, '..')\nfrom utils.checks import capstone_data_cleaning as verify\n\n# Dataset path (provided for convenience)\nMESSY_DATA_PATH = '../datasets/synthetic/messy_data.csv'\n\nprint(\"Checker loaded!\")\nprint(f\"Dataset path: {MESSY_DATA_PATH}\")\nprint(\"\\nNow import the libraries you need and load the dataset.\")"
  },
  {
   "cell_type": "markdown",
   "id": "2ccd47td31r",
   "source": "---\n## Problem 0: Import Libraries and Load Data\n**Difficulty:** Easy\n\n### Concept\nData cleaning requires libraries for data manipulation and visualization. Load the messy dataset to begin.\n\n### Syntax\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline\nnp.random.seed(42)\n\n# Load the messy dataset\nmessy_df = pd.read_csv(MESSY_DATA_PATH)\n```\n\n### Task\n1. Import the required libraries\n2. Load the messy dataset into `messy_df`\n\n### Expected Properties\n- All libraries should be importable\n- `messy_df` should be a DataFrame",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "s08n5o6h0la",
   "source": "# Your solution:\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6f1bxfomk25",
   "source": "# Verification\ncheck.is_true('np' in dir(), \"P0a: NumPy imported\", \"Import numpy as np\")\ncheck.is_true('pd' in dir(), \"P0b: Pandas imported\", \"Import pandas as pd\")\ncheck.is_true('plt' in dir(), \"P0c: Matplotlib imported\", \"Import matplotlib.pyplot as plt\")\ncheck.is_true('os' in dir(), \"P0d: OS module imported\", \"Import os module\")\ncheck.is_true('messy_df' in dir(), \"P0e: Dataset loaded\", \"Load the messy dataset into messy_df\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Initial Data Quality Assessment\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "Before cleaning data, you need to assess its quality. The first step is identifying missing values - how many are in each column and what percentage they represent. This helps prioritize cleaning efforts.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.isnull().sum()  # Count missing values per column\n",
    "(df.isnull().sum() / len(df)) * 100  # Percentage missing\n",
    "\n",
    "# Create summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': df.isnull().sum(),\n",
    "    'missing_pct': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> data = pd.DataFrame({'A': [1, None, 3], 'B': [4, 5, None]})\n",
    ">>> data.isnull().sum()\n",
    "A    1\n",
    "B    1\n",
    ">>> (data.isnull().sum() / len(data)) * 100\n",
    "A    33.33\n",
    "B    33.33\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a DataFrame called `missing_summary` with two columns:\n",
    "- `'missing_count'`: count of missing values per column\n",
    "- `'missing_pct'`: percentage of missing values per column\n",
    "\n",
    "Work with a copy of messy_df called `df`.\n",
    "\n",
    "### Expected Properties\n",
    "- `missing_summary` should be a DataFrame\n",
    "- Should have columns 'missing_count' and 'missing_pct'\n",
    "- Index should match the columns in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "df = messy_df.copy()  # Work with a copy\n",
    "missing_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(missing_summary, pd.DataFrame, \"P1: Type check\")\n",
    "check.contains_column(missing_summary, 'missing_count', \"P1: Has missing_count column\")\n",
    "check.contains_column(missing_summary, 'missing_pct', \"P1: Has missing_pct column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Examine Data Types\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "Understanding data types is crucial for cleaning. Columns might be stored as the wrong type (e.g., numbers as strings, dates as objects). The `dtypes` attribute shows the current data type of each column.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.dtypes          # Returns Series of data types\n",
    "df.info()          # Shows dtypes plus memory usage\n",
    "df['col'].dtype    # Get type of specific column\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> data = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})\n",
    ">>> data.dtypes\n",
    "A     int64\n",
    "B    object\n",
    "```\n",
    "\n",
    "### Task\n",
    "Get the data types of all columns in df and store in `current_dtypes`.\n",
    "\n",
    "### Expected Properties\n",
    "- `current_dtypes` should be a pandas Series\n",
    "- Should have an entry for each column in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "current_dtypes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(current_dtypes, pd.Series, \"P2: Type check\")\n",
    "check.has_length(current_dtypes, len(df.columns), \"P2: Has entry for each column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Remove Duplicate Rows\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "Duplicate rows can skew analysis. They might result from data collection errors or merging issues. Pandas provides methods to detect and remove duplicates.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.duplicated()              # Returns boolean Series marking duplicates\n",
    "df.duplicated().sum()        # Count duplicates\n",
    "df.drop_duplicates()         # Remove duplicates\n",
    "df.drop_duplicates(inplace=True)  # Modify in place\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> data = pd.DataFrame({'A': [1, 2, 1], 'B': [3, 4, 3]})\n",
    ">>> data.duplicated().sum()\n",
    "1\n",
    ">>> data.drop_duplicates()\n",
    "   A  B\n",
    "0  1  3\n",
    "1  2  4\n",
    "```\n",
    "\n",
    "### Task\n",
    "1. Count the number of duplicate rows and store in `duplicates_removed`\n",
    "2. Remove duplicate rows from df\n",
    "\n",
    "### Expected Properties\n",
    "- After removal, df should have no duplicate rows\n",
    "- `duplicates_removed` should be a non-negative integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "duplicates_removed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(duplicates_removed, (int, np.integer), \"P3: Type check\")\n",
    "check.is_true(df.duplicated().sum() == 0, \"P3: No duplicates remain\", \"All duplicates should be removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Clean String Columns\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "String data often has inconsistent formatting - extra whitespace, mixed case, etc. Pandas string methods help standardize text data for consistent analysis.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['col'].str.strip()      # Remove leading/trailing whitespace\n",
    "df['col'].str.lower()      # Convert to lowercase\n",
    "df['col'].str.upper()      # Convert to uppercase\n",
    "df['col'].str.title()      # Title Case\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> names = pd.Series(['  JOHN  ', 'jane', '  BOB'])\n",
    ">>> names.str.strip().str.title()\n",
    "0    John\n",
    "1    Jane\n",
    "2     Bob\n",
    "```\n",
    "\n",
    "### Task\n",
    "For all string (object) columns in df:\n",
    "1. Strip whitespace\n",
    "2. Convert to title case\n",
    "\n",
    "Store the count of string columns cleaned in `string_cols_cleaned`.\n",
    "\n",
    "### Expected Properties\n",
    "- `string_cols_cleaned` should be a positive integer\n",
    "- String columns should have no leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "string_cols_cleaned = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(string_cols_cleaned, (int, np.integer), \"P4: Type check\")\n",
    "check.is_true(string_cols_cleaned > 0, \"P4: At least one string column\", \"Should have cleaned at least one string column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Convert Numeric Columns\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Sometimes numeric data is stored as strings. This prevents mathematical operations. `pd.to_numeric()` converts strings to numbers, with options for handling errors.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "pd.to_numeric(series, errors='raise')    # Raise error on invalid\n",
    "pd.to_numeric(series, errors='coerce')   # Convert invalid to NaN\n",
    "pd.to_numeric(series, errors='ignore')   # Leave invalid unchanged\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> values = pd.Series(['1', '2', 'invalid', '4'])\n",
    ">>> pd.to_numeric(values, errors='coerce')\n",
    "0    1.0\n",
    "1    2.0\n",
    "2    NaN\n",
    "3    4.0\n",
    "```\n",
    "\n",
    "### Task\n",
    "For columns that should be numeric but aren't, convert them using `pd.to_numeric()` with `errors='coerce'`. Store the count of columns converted in `numeric_cols_converted`.\n",
    "\n",
    "### Expected Properties\n",
    "- `numeric_cols_converted` should be a non-negative integer\n",
    "- Converted columns should have numeric dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "numeric_cols_converted = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(numeric_cols_converted, (int, np.integer), \"P5: Type check\")\n",
    "check.is_true(numeric_cols_converted >= 0, \"P5: Non-negative count\", \"Should be a non-negative number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: Fill Missing Numeric Values\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Missing numeric values need to be handled. Common strategies include filling with mean, median (robust to outliers), or mode. The median is often preferred for skewed distributions.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['col'].fillna(value)               # Fill with specific value\n",
    "df['col'].fillna(df['col'].mean())    # Fill with mean\n",
    "df['col'].fillna(df['col'].median())  # Fill with median\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> values = pd.Series([1, 2, None, 4, 5])\n",
    ">>> values.fillna(values.median())\n",
    "0    1.0\n",
    "1    2.0\n",
    "2    3.0  # median\n",
    "3    4.0\n",
    "4    5.0\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fill missing values in all numeric columns with the median of that column. Store the total count of values filled in `values_filled`.\n",
    "\n",
    "### Expected Properties\n",
    "- After filling, numeric columns should have no missing values\n",
    "- `values_filled` should be a non-negative integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "values_filled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(values_filled, (int, np.integer), \"P6: Type check\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numeric_missing = df[numeric_cols].isnull().sum().sum()\n",
    "check.is_true(numeric_missing == 0, \"P6: No missing numeric values\", \"All numeric missing values should be filled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 7: Fill Missing Categorical Values\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "For categorical data, filling with the mode (most frequent value) maintains the distribution. Alternatively, use a placeholder like 'Unknown' to preserve information about missingness.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['col'].mode()[0]           # Get most frequent value\n",
    "df['col'].fillna('Unknown')   # Fill with placeholder\n",
    "df['col'].fillna(df['col'].mode()[0])  # Fill with mode\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> categories = pd.Series(['A', 'B', 'A', None, 'A'])\n",
    ">>> categories.mode()[0]\n",
    "'A'\n",
    ">>> categories.fillna('A')\n",
    "0    A\n",
    "1    B\n",
    "2    A\n",
    "3    A  # filled\n",
    "4    A\n",
    "```\n",
    "\n",
    "### Task\n",
    "Fill missing values in all categorical (object) columns with the mode of that column. If a column has no mode, use 'Unknown'. Store the count of values filled in `cat_filled`.\n",
    "\n",
    "### Expected Properties\n",
    "- After filling, df should have no missing values at all\n",
    "- `cat_filled` should be a non-negative integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "cat_filled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(cat_filled, (int, np.integer), \"P7: Type check\")\n",
    "total_missing = df.isnull().sum().sum()\n",
    "check.is_true(total_missing == 0, \"P7: No missing values\", \"All missing values should be filled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 8: Detect Outliers Using IQR\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Outliers are extreme values that can distort analysis. The Interquartile Range (IQR) method defines outliers as values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR, where Q1 and Q3 are the 25th and 75th percentiles.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "Q1 = df['col'].quantile(0.25)\n",
    "Q3 = df['col'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = (df['col'] < lower_bound) | (df['col'] > upper_bound)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> data = pd.Series([1, 2, 3, 4, 5, 100])  # 100 is an outlier\n",
    ">>> Q1, Q3 = data.quantile([0.25, 0.75])\n",
    ">>> IQR = Q3 - Q1\n",
    ">>> outliers = (data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)\n",
    ">>> outliers.sum()\n",
    "1\n",
    "```\n",
    "\n",
    "### Task\n",
    "For each numeric column, count outliers using the IQR method. Store results in a dictionary `outlier_counts` with column names as keys and counts as values.\n",
    "\n",
    "### Expected Properties\n",
    "- `outlier_counts` should be a dictionary\n",
    "- Should have an entry for each numeric column\n",
    "- All counts should be non-negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "outlier_counts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(outlier_counts, dict, \"P8: Type check\")\n",
    "check.is_true(len(outlier_counts) > 0, \"P8: Has entries\", \"Should have outlier counts for numeric columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 9: Handle Outliers with Winsorization\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Winsorization caps extreme values at upper and lower bounds rather than removing them. This preserves row count while reducing outlier impact. Values below the lower bound are set to the bound, and values above the upper bound are capped.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "# Method 1: Manual clipping\n",
    "df['col'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Method 2: Using numpy\n",
    "np.clip(df['col'], lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> values = pd.Series([1, 2, 3, 4, 100])\n",
    ">>> values.clip(lower=1, upper=10)\n",
    "0     1\n",
    "1     2\n",
    "2     3\n",
    "3     4\n",
    "4    10  # capped from 100\n",
    "```\n",
    "\n",
    "### Task\n",
    "For each numeric column, cap outliers at the IQR bounds (Q1 - 1.5×IQR and Q3 + 1.5×IQR). Store the total count of values capped in `values_capped`.\n",
    "\n",
    "### Expected Properties\n",
    "- `values_capped` should be a non-negative integer\n",
    "- No values should remain outside the IQR bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "values_capped = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(values_capped, (int, np.integer), \"P9: Type check\")\n",
    "check.is_true(values_capped >= 0, \"P9: Non-negative count\", \"Should be a non-negative number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 10: Standardize Category Values\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Categorical data often has inconsistent formatting ('yes', 'Yes', 'YES'). Standardizing to a consistent format ensures proper grouping and analysis.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df['col'].str.strip().str.title()   # Clean and title case\n",
    "df['col'].str.lower()                # All lowercase\n",
    "df['col'].nunique()                  # Count unique values\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> categories = pd.Series(['yes', 'YES', ' Yes ', 'no'])\n",
    ">>> categories.nunique()\n",
    "4  # Before standardization\n",
    ">>> categories = categories.str.strip().str.title()\n",
    ">>> categories.nunique()\n",
    "2  # After: 'Yes' and 'No'\n",
    "```\n",
    "\n",
    "### Task\n",
    "For all categorical columns, standardize values to title case (already done in Problem 4, but ensure it's consistent). Track unique values before and after. Store in dictionaries `unique_before` and `unique_after`.\n",
    "\n",
    "### Expected Properties\n",
    "- Both should be dictionaries\n",
    "- `unique_after` values should be less than or equal to `unique_before`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "unique_before = None\n",
    "unique_after = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(unique_before, dict, \"P10a: unique_before is dict\")\n",
    "check.is_type(unique_after, dict, \"P10b: unique_after is dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 11: Create Data Cleaning Report\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Documentation is key in data cleaning. A cleaning report summarizes what was done, helping others understand the transformations and assess data quality.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "report = {\n",
    "    'metric_name': value,\n",
    "    'rows_before': len(original_df),\n",
    "    'rows_after': len(cleaned_df),\n",
    "    'missing_pct_before': (original_df.isnull().sum().sum() / original_df.size) * 100\n",
    "}\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> report = {\n",
    "...     'original_rows': 1000,\n",
    "...     'final_rows': 950,\n",
    "...     'duplicates_removed': 50\n",
    "... }\n",
    ">>> for key, value in report.items():\n",
    "...     print(f\"{key}: {value}\")\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a dictionary `cleaning_report` with these keys:\n",
    "- 'original_rows': row count before cleaning\n",
    "- 'final_rows': row count after cleaning\n",
    "- 'duplicates_removed': count from Problem 3\n",
    "- 'missing_values_filled': sum of values_filled and cat_filled\n",
    "- 'outliers_capped': count from Problem 9\n",
    "- 'original_missing_pct': percentage of missing values in messy_df\n",
    "- 'final_missing_pct': percentage of missing values in df (should be 0)\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a dictionary\n",
    "- Should have all required keys\n",
    "- 'final_missing_pct' should be 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "cleaning_report = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(cleaning_report, dict, \"P11: Type check\")\n",
    "check.contains(cleaning_report.keys(), 'original_rows', \"P11a: Has original_rows\")\n",
    "check.contains(cleaning_report.keys(), 'final_missing_pct', \"P11b: Has final_missing_pct\")\n",
    "check.is_true(cleaning_report.get('final_missing_pct', -1) == 0.0, \"P11c: No missing values\", \"final_missing_pct should be 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 12: Validate Final Data Types\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "After cleaning, verify that columns have appropriate data types. This ensures the data is ready for analysis and prevents type-related errors.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.dtypes                  # Check all types\n",
    "df['col'].astype('int64')  # Convert type if needed\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.dtypes\n",
    "age       int64\n",
    "name     object\n",
    "score   float64\n",
    "```\n",
    "\n",
    "### Task\n",
    "Get the final data types of all columns and store in `final_dtypes`.\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a pandas Series\n",
    "- Should have an entry for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "final_dtypes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(final_dtypes, pd.Series, \"P12: Type check\")\n",
    "check.has_length(final_dtypes, len(df.columns), \"P12: Has entry for each column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 13: Visualize Data Quality Improvement\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "Visualizations communicate cleaning impact effectively. Comparing before/after metrics shows the improvement in data quality.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].bar(x, before_values, label='Before')\n",
    "axes[0].bar(x, after_values, alpha=0.7, label='After')\n",
    "axes[1].hist(cleaned_data)\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> fig, ax = plt.subplots()\n",
    ">>> ax.bar(['Before', 'After'], [missing_before, missing_after])\n",
    ">>> ax.set_ylabel('Missing Values')\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a figure with 2 subplots:\n",
    "- Left: Compare missing values before and after\n",
    "- Right: Show distribution of a numeric column after cleaning\n",
    "\n",
    "### Expected Properties\n",
    "- `axes` should have 2 elements\n",
    "- Both plots should be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "fig = None\n",
    "axes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_not_none(axes, \"P13: Axes created\")\n",
    "check.has_length(axes, 2, \"P13: Has 2 subplots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 14: Save Cleaned Data\n",
    "**Difficulty:** Easy\n",
    "\n",
    "### Concept\n",
    "After cleaning, save the data for future use. CSV format is widely compatible, but you can also use pickle for preserving exact Python data types.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "df.to_csv('filepath.csv', index=False)     # Save to CSV\n",
    "df.to_pickle('filepath.pkl')                # Save to pickle\n",
    "os.path.exists('filepath.csv')              # Check if file exists\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> df.to_csv('cleaned_data.csv', index=False)\n",
    ">>> os.path.exists('cleaned_data.csv')\n",
    "True\n",
    "```\n",
    "\n",
    "### Task\n",
    "Save df to '../datasets/synthetic/cleaned_data.csv'. Store True/False success status in `save_success`.\n",
    "\n",
    "### Expected Properties\n",
    "- File should be created at the specified path\n",
    "- `save_success` should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "save_success = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(save_success, bool, \"P14: Type check\")\n",
    "check.is_true(save_success == True, \"P14: File saved\", \"File should be saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 15: Create Final Summary\n",
    "**Difficulty:** Medium\n",
    "\n",
    "### Concept\n",
    "A comprehensive summary provides complete documentation of the final dataset, including shape, columns, data types, and basic statistics.\n",
    "\n",
    "### Syntax\n",
    "```python\n",
    "summary = {\n",
    "    'shape': df.shape,\n",
    "    'columns': list(df.columns),\n",
    "    'dtypes': dict(df.dtypes),\n",
    "    'memory_usage': df.memory_usage(deep=True).sum()\n",
    "}\n",
    "```\n",
    "\n",
    "### Example\n",
    "```python\n",
    ">>> summary = {\n",
    "...     'shape': (1000, 5),\n",
    "...     'columns': ['A', 'B', 'C', 'D', 'E'],\n",
    "...     'missing_values': 0\n",
    "... }\n",
    "```\n",
    "\n",
    "### Task\n",
    "Create a dictionary `final_summary` with:\n",
    "- 'shape': df.shape\n",
    "- 'columns': list of column names\n",
    "- 'dtypes': dictionary of column: dtype\n",
    "- 'missing_values': total missing values (should be 0)\n",
    "- 'numeric_summary': df.describe().to_dict()\n",
    "- 'memory_usage': total memory usage in bytes\n",
    "\n",
    "### Expected Properties\n",
    "- Should be a dictionary\n",
    "- 'missing_values' should be 0\n",
    "- Should have all required keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:\n",
    "final_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "check.is_type(final_summary, dict, \"P15: Type check\")\n",
    "check.contains(final_summary.keys(), 'shape', \"P15a: Has shape\")\n",
    "check.contains(final_summary.keys(), 'missing_values', \"P15b: Has missing_values\")\n",
    "check.is_true(final_summary.get('missing_values', -1) == 0, \"P15c: No missing values\", \"missing_values should be 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Lessons Learned\n",
    "\n",
    "Document the key lessons from this data cleaning exercise:\n",
    "\n",
    "1. **Missing Data**: How did you handle different types of missing values?\n",
    "2. **Duplicates**: What impact did removing duplicates have?\n",
    "3. **Data Types**: Why is correct data typing important?\n",
    "4. **Outliers**: What strategy worked best for outliers in your data?\n",
    "5. **Standardization**: How did standardization improve data quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "check.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}